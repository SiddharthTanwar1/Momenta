{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f51363a-80a8-4607-9bcb-786bf1d0a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, LSTM, Dense, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define directories\n",
    "base_dir = \"/Users/siddharthtanwar/Documents/York/momenta/for-original\"\n",
    "train_fake_dir = os.path.join(base_dir, \"training/fake\")\n",
    "train_real_dir = os.path.join(base_dir, \"training/real\")\n",
    "test_fake_dir = os.path.join(base_dir, \"testing/fake\")\n",
    "test_real_dir = os.path.join(base_dir, \"testing/real\")\n",
    "val_fake_dir = os.path.join(base_dir, \"validation/fake\")\n",
    "val_real_dir = os.path.join(base_dir, \"validation/real\")\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(file_path, model_type):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    if model_type == 1:\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        pitch, _ = librosa.piptrack(y=y, sr=sr)\n",
    "        return np.hstack((mfcc.mean(axis=1), pitch.mean(axis=1)))\n",
    "    elif model_type == 2:\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        return [rms.mean()]  # Return a single value\n",
    "    elif model_type == 3:\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        return [zcr.mean()]  # Return a single value\n",
    "\n",
    "\n",
    "# Function to load data\n",
    "def load_data(fake_dir, real_dir, model_type):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Add tqdm for fake files\n",
    "    for file in tqdm(os.listdir(fake_dir), desc=f\"Loading fake data for Model {model_type}\"):\n",
    "        if file.endswith('.wav'):\n",
    "            features.append(extract_features(os.path.join(fake_dir, file), model_type))\n",
    "            labels.append(0)\n",
    "    \n",
    "    # Add tqdm for real files\n",
    "    for file in tqdm(os.listdir(real_dir), desc=f\"Loading real data for Model {model_type}\"):\n",
    "        if file.endswith('.wav'):\n",
    "            features.append(extract_features(os.path.join(real_dir, file), model_type))\n",
    "            labels.append(1)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "80b56939-88d0-45a8-aa30-829ee0b70a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for Model 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading fake data for Model 1: 100%|████████| 26941/26941 [00:17<00:00, 1517.18it/s]\n",
      "Loading real data for Model 1: 100%|██████████| 26941/26941 [05:23<00:00, 83.19it/s]\n",
      "Loading fake data for Model 1: 100%|███████████| 2370/2370 [00:16<00:00, 147.90it/s]\n",
      "Loading real data for Model 1: 100%|███████████| 2264/2264 [00:18<00:00, 124.26it/s]\n",
      "Loading fake data for Model 1: 100%|██████████| 5400/5400 [00:04<00:00, 1229.76it/s]\n",
      "Loading real data for Model 1: 100%|████████████| 5400/5400 [01:04<00:00, 83.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data for each model\n",
    "print(\"Loading data for Model 1...\")\n",
    "X_train_1, y_train_1 = load_data(train_fake_dir, train_real_dir, 1)\n",
    "X_test_1, y_test_1 = load_data(test_fake_dir, test_real_dir, 1)\n",
    "X_val_1, y_val_1 = load_data(val_fake_dir, val_real_dir, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6455206-b066-4f80-93ef-80a9873ecaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for Model 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading fake data for Model 2: 100%|████████| 26941/26941 [00:04<00:00, 6728.91it/s]\n",
      "Loading real data for Model 2: 100%|█████████| 26941/26941 [01:08<00:00, 393.65it/s]\n",
      "Loading fake data for Model 2: 100%|███████████| 2370/2370 [00:03<00:00, 723.15it/s]\n",
      "Loading real data for Model 2: 100%|███████████| 2264/2264 [00:03<00:00, 633.87it/s]\n",
      "Loading fake data for Model 2: 100%|██████████| 5400/5400 [00:00<00:00, 5920.77it/s]\n",
      "Loading real data for Model 2: 100%|███████████| 5400/5400 [00:14<00:00, 384.29it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data for Model 2...\")\n",
    "X_train_2, y_train_2 = load_data(train_fake_dir, train_real_dir, 2)\n",
    "X_test_2, y_test_2 = load_data(test_fake_dir, test_real_dir, 2)\n",
    "X_val_2, y_val_2 = load_data(val_fake_dir, val_real_dir, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa1764b3-b85b-4876-a9b6-f98fcfa9ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for Model 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading fake data for Model 3: 100%|████████| 26941/26941 [00:05<00:00, 4848.46it/s]\n",
      "Loading real data for Model 3: 100%|█████████| 26941/26941 [01:24<00:00, 317.89it/s]\n",
      "Loading fake data for Model 3: 100%|███████████| 2370/2370 [00:04<00:00, 530.59it/s]\n",
      "Loading real data for Model 3: 100%|███████████| 2264/2264 [00:04<00:00, 455.87it/s]\n",
      "Loading fake data for Model 3: 100%|██████████| 5400/5400 [00:01<00:00, 4761.20it/s]\n",
      "Loading real data for Model 3: 100%|███████████| 5400/5400 [00:16<00:00, 323.46it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data for Model 3...\")\n",
    "X_train_3, y_train_3 = load_data(train_fake_dir, train_real_dir, 3)\n",
    "X_test_3, y_test_3 = load_data(test_fake_dir, test_real_dir, 3)\n",
    "X_val_3, y_val_3 = load_data(val_fake_dir, val_real_dir, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d3327263-e7fe-4728-888b-eed3ef528c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 242ms/step - accuracy: 0.7789 - loss: 0.3855 - val_accuracy: 0.9880 - val_loss: 0.0459 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 322ms/step - accuracy: 0.9787 - loss: 0.0546 - val_accuracy: 0.9967 - val_loss: 0.0131 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 307ms/step - accuracy: 0.9946 - loss: 0.0147 - val_accuracy: 0.9960 - val_loss: 0.0118 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 286ms/step - accuracy: 0.9719 - loss: 0.0692 - val_accuracy: 0.9842 - val_loss: 0.0501 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 301ms/step - accuracy: 0.9919 - loss: 0.0232 - val_accuracy: 0.9974 - val_loss: 0.0102 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 298ms/step - accuracy: 0.9943 - loss: 0.0156 - val_accuracy: 0.9964 - val_loss: 0.0145 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 284ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 0.9964 - val_loss: 0.0136 - learning_rate: 2.5000e-04\n",
      "\n",
      "=== Test Set Evaluation ===\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step\n",
      "\n",
      "Validation Set Performance with Threshold 0.395:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       675\n",
      "           1       1.00      1.00      1.00      5400\n",
      "\n",
      "    accuracy                           1.00      6075\n",
      "   macro avg       0.99      1.00      0.99      6075\n",
      "weighted avg       1.00      1.00      1.00      6075\n",
      "\n",
      "Validation Confusion Matrix:\n",
      " [[ 672    3]\n",
      " [  12 5388]]\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step\n",
      "\n",
      "Test-Optimal Threshold: 0.437\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D  # Add this import\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Reshape, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model_1(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),  # Explicit input layer\n",
    "        \n",
    "        # Modified CNN Layers - using (3,1) kernels for your (1038,1,1) input\n",
    "        Conv2D(32, (3,1), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,1)),  # Only pooling along time dimension\n",
    "        \n",
    "        Conv2D(64, (3,1), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2,1)),\n",
    "        \n",
    "        Conv2D(64, (3,1), activation='relu', padding='same'),\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        Reshape((-1, 64)),  # Flatten spatial dimensions\n",
    "        \n",
    "        # BiLSTM Layers\n",
    "        Bidirectional(LSTM(32, return_sequences=True)),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        \n",
    "        # Output\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Data preparation with validation set included\n",
    "X_train_1 = X_train_1.reshape(X_train_1.shape[0], X_train_1.shape[1], 1, 1)\n",
    "X_test_1 = X_test_1.reshape(X_test_1.shape[0], X_test_1.shape[1], 1, 1)\n",
    "X_val_1 = X_val_1.reshape(X_val_1.shape[0], X_val_1.shape[1], 1, 1)\n",
    "\n",
    "# SMOTE application (only on training data)\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.7)\n",
    "X_train_flat = X_train_1.reshape(X_train_1.shape[0], -1)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_flat, y_train_1)\n",
    "X_train_res = X_train_res.reshape(-1, X_train_1.shape[1], 1, 1)\n",
    "\n",
    "# Class weights calculation\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train_1)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Model training with proper validation\n",
    "model_1 = create_model_1((1038, 1, 1))\n",
    "history_1 = model_1.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_1, y_val_1),  # Use explicit validation data\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=2, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== Test Set Evaluation ===\")\n",
    "\n",
    "# 1. First verify threshold on validation set\n",
    "y_val_proba = model_1.predict(X_val_1, verbose=1)\n",
    "y_val_pred = (y_val_proba > best_threshold).astype(int)\n",
    "print(\"\\nValidation Set Performance with Threshold %.3f:\" % best_threshold)\n",
    "print(classification_report(y_val_1, y_val_pred))\n",
    "print(\"Validation Confusion Matrix:\\n\", confusion_matrix(y_val_1, y_val_pred))\n",
    "\n",
    "# 2. Then evaluate on test set\n",
    "y_test_proba = model_1.predict(X_test_1, verbose=1)\n",
    "y_test_pred = (y_test_proba > best_threshold).astype(int)\n",
    "\n",
    "# 3. Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test_1, y_test_pred)\n",
    "test_f1 = f1_score(y_test_1, y_test_pred)\n",
    "test_precision = precision_score(y_test_1, y_test_pred)\n",
    "test_recall = recall_score(y_test_1, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "# 4. Optionally find test-optimal threshold if needed\n",
    "test_thresholds = np.linspace(0.1, 0.5, 20)\n",
    "test_opt_threshold = max(test_thresholds, \n",
    "                        key=lambda t: f1_score(y_test_1, (y_test_proba > t).astype(int)))\n",
    "print(\"\\nTest-Optimal Threshold: %.3f\" % test_opt_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2b07f75-a9e0-40c5-bd4b-2a8d5576f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhancing Features: 100%|██████████████| 30204/30204 [00:22<00:00, 1342.32samples/s]\n",
      "Enhancing Features: 100%|████████████████| 6075/6075 [00:03<00:00, 1948.47samples/s]\n",
      "Enhancing Features: 100%|████████████████| 4634/4634 [00:02<00:00, 1943.65samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Optimizing Threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for Best Threshold: 100%|████████████████| 30/30 [00:00<00:00, 167.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "Optimal Threshold: 0.400\n",
      "Test Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.73      2370\n",
      "           1       0.75      0.59      0.66      2264\n",
      "\n",
      "    accuracy                           0.70      4634\n",
      "   macro avg       0.71      0.70      0.70      4634\n",
      "weighted avg       0.71      0.70      0.70      4634\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1919  451]\n",
      " [ 936 1328]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 1. Feature Engineering with Spectral Features\n",
    "def add_spectral_features(X):\n",
    "    \"\"\"Enhance feature set with spectral centroid-based features.\"\"\"\n",
    "    X_df = pd.DataFrame(X.copy())\n",
    "\n",
    "    for i in tqdm(range(X_df.shape[0]), desc=\"Enhancing Features\", unit=\"samples\"):\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            X_df.loc[i, 'feature_var'] = np.nanvar(X_df.iloc[i])\n",
    "            X_df.loc[i, 'feature_range'] = np.nanmax(X_df.iloc[i]) - np.nanmin(X_df.iloc[i])\n",
    "            X_df.loc[i, 'feature_entropy'] = -np.nansum(X_df.iloc[i] * np.log1p(X_df.iloc[i]))\n",
    "            X_df.loc[i, 'feature_rolloff'] = np.nanpercentile(X_df.iloc[i], 85)\n",
    "\n",
    "    X_df.columns = X_df.columns.astype(str)\n",
    "    return X_df.values\n",
    "\n",
    "# 2. Data Preparation\n",
    "def prepare_data(X_train, X_val, X_test):\n",
    "    X_train_enhanced = add_spectral_features(X_train)\n",
    "    X_val_enhanced = add_spectral_features(X_val)\n",
    "    X_test_enhanced = add_spectral_features(X_test)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train_enhanced)\n",
    "    X_val_imputed = imputer.transform(X_val_enhanced)\n",
    "    X_test_imputed = imputer.transform(X_test_enhanced)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_val_scaled = scaler.transform(X_val_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n",
    "#Assuming X_train_2, y_train_2, X_val_2, y_val_2, X_test_2, y_test_2 are already loaded.\n",
    "X_train_scaled, X_val_scaled, X_test_scaled = prepare_data(X_train_2, X_val_2, X_test_2)\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.7, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train_2) #important, scaled data\n",
    "\n",
    "# 2. Logistic Regression\n",
    "model_lr = LogisticRegression(class_weight={0: 1, 1: 1.6}, random_state=42, solver='liblinear')\n",
    "\n",
    "# 3. Train Logistic Regression\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "model_lr.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 4. Predict probabilities for threshold optimization\n",
    "y_val_proba = model_lr.predict_proba(X_val_scaled)[:, 1] #predict probability of class 1\n",
    "\n",
    "# 5. Threshold optimization\n",
    "best_threshold = 0.5\n",
    "best_score = 0\n",
    "\n",
    "print(\"\\nOptimizing Threshold...\")\n",
    "for t in tqdm(np.linspace(0.25, 0.4, 30), desc=\"Searching for Best Threshold\"): #adjusted range\n",
    "    score = (precision_score(y_val_2, (y_val_proba > t).astype(int), zero_division=0) *\n",
    "             min(recall_score(y_val_2, (y_val_proba > t).astype(int), zero_division=0),\n",
    "                 recall_score(1 - y_val_2, 1 - (y_val_proba > t).astype(int), zero_division=0)))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold = t\n",
    "\n",
    "# 6. Final predictions on the test set\n",
    "y_test_pred = (model_lr.predict_proba(X_test_scaled)[:, 1] > best_threshold).astype(int)\n",
    "\n",
    "# 7. Results\n",
    "print(\"\\n=== Logistic Regression ===\")\n",
    "print(f\"Optimal Threshold: {best_threshold:.3f}\")\n",
    "print(\"Test Performance:\")\n",
    "print(classification_report(y_test_2, y_test_pred, zero_division=0))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_2, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "09f9d985-ea20-45b1-aef3-7819b00dfe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409us/step - accuracy: 0.6437 - loss: 0.7318 - val_accuracy: 0.7756 - val_loss: 0.5238\n",
      "Epoch 2/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - accuracy: 0.6508 - loss: 0.6116 - val_accuracy: 0.7310 - val_loss: 0.5429\n",
      "Epoch 3/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - accuracy: 0.6581 - loss: 0.6029 - val_accuracy: 0.8695 - val_loss: 0.6271\n",
      "Epoch 4/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - accuracy: 0.6685 - loss: 0.6215 - val_accuracy: 0.7903 - val_loss: 0.4652\n",
      "Epoch 5/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - accuracy: 0.6721 - loss: 0.6057 - val_accuracy: 0.7715 - val_loss: 0.5203\n",
      "Epoch 6/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381us/step - accuracy: 0.6616 - loss: 0.6005 - val_accuracy: 0.7621 - val_loss: 0.4747\n",
      "Epoch 7/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - accuracy: 0.6503 - loss: 0.6049 - val_accuracy: 0.7826 - val_loss: 0.5424\n",
      "Epoch 8/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - accuracy: 0.6733 - loss: 0.5932 - val_accuracy: 0.6859 - val_loss: 0.6429\n",
      "Epoch 9/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - accuracy: 0.6519 - loss: 0.6197 - val_accuracy: 0.8010 - val_loss: 0.5040\n",
      "Epoch 10/10\n",
      "\u001b[1m1432/1432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - accuracy: 0.6679 - loss: 0.6123 - val_accuracy: 0.8283 - val_loss: 0.4596\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259us/step\n",
      "\n",
      "Validation Results (Simple MLP):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.39      0.34       675\n",
      "           1       0.92      0.88      0.90      5400\n",
      "\n",
      "    accuracy                           0.83      6075\n",
      "   macro avg       0.61      0.64      0.62      6075\n",
      "weighted avg       0.85      0.83      0.84      6075\n",
      "\n",
      "Validation Confusion Matrix:\n",
      "[[ 265  410]\n",
      " [ 633 4767]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 2. Data Preparation (No Feature Enhancement)\n",
    "def prepare_data_lite(X_train, X_val, X_test):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_val_imputed = imputer.transform(X_val)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_val_scaled = scaler.transform(X_val_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n",
    "# Load the data (assuming you have X_train_3, y_train_3, etc.)\n",
    "X_train_scaled, X_val_scaled, X_test_scaled = prepare_data_lite(X_train_3, X_val_3, X_test_3)\n",
    "\n",
    "# Apply SMOTE to the training set only\n",
    "smote = SMOTE(sampling_strategy=0.7, random_state=42)  # Adjust sampling_strategy as needed\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, np.array(y_train_3))\n",
    "\n",
    "# Define Simple MLP Model\n",
    "def create_simple_mlp(input_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "\n",
    "    # First Hidden Layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))  # Dropout for regularization\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation='relu'))  # Binary classification\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and train the MLP model\n",
    "input_dim = X_train_scaled.shape[1]  # Number of features\n",
    "mlp_model = create_simple_mlp(input_dim)\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    validation_data=(X_val_scaled, y_val_3),\n",
    "    epochs=10,  # Fewer epochs for faster training\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred_mlp = (mlp_model.predict(X_val_scaled) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Validation results\n",
    "print(\"\\nValidation Results (Simple MLP):\")\n",
    "print(classification_report(y_val_3, y_val_pred_mlp, zero_division=0))\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val_3, y_val_pred_mlp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadfffc-7323-475d-8010-5b19a29b5c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
